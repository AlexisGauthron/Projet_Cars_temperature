# √âtat de l'Art - Benchmarks de Reconnaissance d'√âmotions Faciales

> Documentation mise √† jour : Janvier 2025

## Table des Mati√®res

1. [Vue d'ensemble des Datasets](#vue-densemble-des-datasets)
2. [FER2013 Benchmark](#fer2013-benchmark)
3. [AffectNet Benchmark](#affectnet-benchmark)
4. [RAF-DB Benchmark](#raf-db-benchmark)
5. [CK+ Benchmark](#ck-benchmark)
6. [Comparaison Multi-Dataset](#comparaison-multi-dataset)
7. [Mod√®les State-of-the-Art](#mod√®les-state-of-the-art)
8. [R√©f√©rences](#r√©f√©rences)

---

## Vue d'ensemble des Datasets

| Dataset | Images | Classes | R√©solution | Type | Ann√©e |
|---------|--------|---------|------------|------|-------|
| **FER2013** | 35,887 | 7 | 48√ó48 grayscale | Wild | 2013 |
| **FER+** | 35,887 | 8 | 48√ó48 grayscale | Wild | 2016 |
| **AffectNet** | 1,000,000+ | 8 | Variable RGB | Wild | 2017 |
| **RAF-DB** | ~30,000 | 7/12 | Variable RGB | Wild | 2017 |
| **CK+** | 593 s√©q. | 7 | 640√ó490 | Lab | 2010 |
| **ExpW** | 91,793 | 7 | Variable RGB | Wild | 2015 |
| **JAFFE** | 213 | 7 | 256√ó256 | Lab | 1998 |

### Classes d'√©motions standard (7 classes FER2013)
- üò† **Angry** (Col√®re)
- ü§¢ **Disgust** (D√©go√ªt)
- üò® **Fear** (Peur)
- üòä **Happy** (Joie)
- üò¢ **Sad** (Tristesse)
- üò≤ **Surprise**
- üòê **Neutral** (Neutre)

---

## FER2013 Benchmark

### Informations du Dataset
- **Taille**: 35,887 images (28,709 train / 3,589 public test / 3,589 private test)
- **R√©solution**: 48√ó48 pixels, niveaux de gris
- **Source**: Images web avec √©tiquetage crowdsourc√©
- **Pr√©cision humaine**: ~65.5% (¬±5%)

### D√©fis du Dataset
- Forte variation intra-classe (pose, √©clairage, occlusion)
- Faible s√©parabilit√© inter-classe (disgust vs. anger, fear vs. sadness)
- D√©s√©quilibre des classes (disgust et fear sous-repr√©sent√©s)
- Bruit dans les labels (~10-15% d'erreurs d'annotation)

### Leaderboard FER2013 (Top Mod√®les)

| Rang | Mod√®le | Accuracy | Ann√©e | Notes |
|------|--------|----------|-------|-------|
| 1 | **Synthetic Augmentation (SD)** | **96.47%** | 2024 | +Data synth√©tique Stable Diffusion |
| 2 | Gabor+LBP AlexNet | 98.10%* | 2024 | *Features engineering |
| 3 | MAEL-FER | 85.78% | 2024 | Multi-Attention Enhanced |
| 4 | POSTER++ | 80.76% | 2023 | Transformer-based |
| 5 | DAN | 79.27% | 2021 | Distract Your Attention |
| 6 | EmoNeXt-XLarge | 76.12% | 2023 | ConvNeXt-based |
| 7 | Segmentation VGG-19 | 75.97% | 2023 | - |
| 8 | CNNs + BOVW | 75.42% | 2022 | Bag of Visual Words |
| 9 | EmoNeXt-Large | 75.57% | 2023 | - |
| 10 | LHC-Net | 74.42% | 2022 | - |
| 11 | **RMN** | 74.14% | 2021 | Residual Masking Network |
| 12 | EmoNeXt-Small | 74.33% | 2023 | - |
| 13 | VGGNet (optimized) | 73.28% | 2021 | State-of-the-art sans extra data |
| 14 | SE-Net50 | 72.50% | 2020 | Squeeze-and-Excitation |
| 15 | Ad-Corre | 72.03% | 2021 | - |

### Mod√®les Classiques (R√©f√©rence)

| Mod√®le | Accuracy | FPS* | Notes |
|--------|----------|------|-------|
| VGG16 | 73.28% | ~15 | Baseline robuste |
| ResNet50 | 73.20% | ~20 | - |
| Inception | 71.60% | ~25 | - |
| Deep Emotion | 70.02% | ~30 | L√©ger |
| GoogleNet | 65.20% | ~35 | - |
| **DeepFace** | ~70-75% | **238** | Production-ready |
| **ViT-FER** | ~71-73% | ~18 | Transformer |

*FPS approximatif sur CPU

---

## AffectNet Benchmark

### Informations du Dataset
- **Taille**: 1,000,000+ images (~450,000 manuellement annot√©es)
- **Classes**: 8 √©motions (inclut Contempt)
- **Annotations**: Valence/Arousal + cat√©gories discr√®tes
- **Split**: 287,401 train / 4,000 validation

### D√©fis Sp√©cifiques
- Dataset partiellement annot√© automatiquement
- ~60% d'accord inter-annotateur seulement
- Grande variabilit√© des conditions "in-the-wild"

### Leaderboard AffectNet (8 classes)

| Rang | Mod√®le | Accuracy | Ann√©e |
|------|--------|----------|-------|
| 1 | Data-Centric Approach | **89.17%** | 2023 |
| 2 | MAEL-FER | 69.08% | 2024 |
| 3 | SFER-MDFAE | 67.86% | 2024 |
| 4 | EfficientNet-B2 | ~66% | 2022 |
| 5 | FCCA | 65.51% | 2023 |
| 6 | ResNet-50 baseline | ~58-60% | - |

### R√©sultats par √âmotion (AffectNet-7)

| √âmotion | Accuracy Moyenne | Difficult√© |
|---------|-----------------|------------|
| Happy | ~85% | Facile |
| Surprise | ~75% | Moyen |
| Neutral | ~70% | Moyen |
| Sad | ~55% | Difficile |
| Angry | ~50% | Difficile |
| Fear | ~45% | Tr√®s difficile |
| Disgust | ~40% | Tr√®s difficile |

---

## RAF-DB Benchmark

### Informations du Dataset
- **Taille**: ~30,000 images (12,271 train / 3,068 test)
- **Classes**: 7 basiques + 12 compos√©es
- **Annotation**: 40 annotateurs par image
- **Qualit√©**: Haute qualit√©, diversit√© r√©elle

### Leaderboard RAF-DB

| Rang | Mod√®le | Accuracy | Ann√©e |
|------|--------|----------|-------|
| 1 | **POSTER++** | **92.21%** | 2023 |
| 2 | POSTER | 92.05% | 2022 |
| 3 | SFER-MDFAE | 92.93% | 2024 |
| 4 | ResNet50+CBAM+TCN | 91.86% | 2024 |
| 5 | FCCA | 91.30% | 2023 |
| 6 | EAC | 90.35% | 2022 |
| 7 | MANet | ~89% | 2021 |
| 8 | VGG16 (improved) | 87.84% | 2023 |
| 9 | FARNet | 87.65% | 2023 |
| 10 | SCN | 87.03% | 2020 |
| 11 | RAN | 86.90% | 2020 |
| 12 | **CLCM (Lightweight)** | 84.00% | 2024 |

---

## CK+ Benchmark

### Informations du Dataset
- **Taille**: 593 s√©quences vid√©o (327 avec labels √©motions)
- **Sujets**: 123 personnes (18-50 ans)
- **Type**: Expressions pos√©es, progression neutre‚Üíapex
- **Environnement**: Contr√¥l√© (laboratoire)

### Leaderboard CK+

| Rang | Mod√®le | Accuracy | Notes |
|------|--------|----------|-------|
| 1 | **AA-DCN** | **99.26%** | 2024 |
| 2 | MAEL-FER | 96.98% | 2024 |
| 3 | Combined Training | 94.70% | 2024 |
| 4 | SFER-MDFAE | ~95% | 2024 |
| 5 | VGG-based | ~94% | - |
| 6 | ResNet-50 | ~93% | - |

> ‚ö†Ô∏è **Note**: CK+ est un dataset de laboratoire avec des expressions pos√©es.
> Les performances √©lev√©es (~95-100%) ne se transf√®rent pas aux conditions r√©elles.

---

## Comparaison Multi-Dataset

### Performance des Mod√®les R√©cents sur Plusieurs Datasets

| Mod√®le | FER2013 | AffectNet | RAF-DB | CK+ | Ann√©e |
|--------|---------|-----------|--------|-----|-------|
| **MAEL-FER** | 85.78% | 69.08% | 94.83% | 96.98% | 2024 |
| **SFER-MDFAE** | 76.18% | 67.86% | 92.93% | - | 2024 |
| **POSTER++** | 80.76% | - | 92.21% | - | 2023 |
| **DAN** | 79.27% | - | 89.70% | - | 2021 |
| **EAC** | ~78% | - | 90.35% | - | 2022 |
| **SCN** | ~75% | - | 87.03% | - | 2020 |
| **RAN** | ~74% | - | 86.90% | - | 2020 |

### Classement des Datasets par Difficult√©

1. **AffectNet** (le plus difficile) - ~60-70% accuracy
2. **FER2013** - ~70-85% accuracy
3. **RAF-DB** - ~85-92% accuracy
4. **CK+** (le plus facile) - ~95-99% accuracy

---

## Mod√®les State-of-the-Art

### Top Architectures 2024-2025

#### 1. POSTER++ (Transformer-based)
- **Architecture**: Vision Transformer + Cross-Attention
- **Points forts**: Attention multi-√©chelle, robuste aux occlusions
- **FER2013**: 80.76% | **RAF-DB**: 92.21%

#### 2. MAEL-FER (Multi-Attention Enhanced)
- **Architecture**: CNN + Multi-head Attention
- **Points forts**: Meilleure g√©n√©ralisation cross-dataset
- **Multi-dataset**: Performances √©quilibr√©es

#### 3. EAC (Erasing Attention Consistency)
- **Architecture**: ResNet + Attention flipping
- **Points forts**: Robuste au bruit de labels
- **RAF-DB**: 90.35%

#### 4. RMN (Residual Masking Network)
- **Architecture**: CNN + Residual Masking
- **Points forts**: Bon ratio accuracy/vitesse
- **FER2013**: 74.14%

#### 5. HSEmotion (EfficientNet-based)
- **Architecture**: EfficientNet-B0/B2
- **Points forts**: Tr√®s rapide, mobile-friendly
- **Vitesse**: ~50+ FPS

### Architectures par Cas d'Usage

| Cas d'Usage | Mod√®le Recommand√© | Accuracy | Vitesse |
|-------------|-------------------|----------|---------|
| **Production haute vitesse** | DeepFace | ~75% | ‚ö°‚ö°‚ö° |
| **Mobile/Edge** | HSEmotion-ONNX | ~70% | ‚ö°‚ö°‚ö° |
| **Meilleure accuracy** | POSTER++ | ~92% | ‚ö° |
| **√âquilibr√©** | ViT-FER | ~73% | ‚ö°‚ö° |
| **Recherche** | MAEL-FER | Variable | ‚ö° |

---

## Tendances et Insights

### √âvolution des Performances (FER2013)

```
2013: 71.2% (Kaggle winner)
2017: 73.0% (VGG optimis√©)
2020: 74.0% (RMN, SCN)
2022: 76.0% (EmoNeXt)
2023: 80.7% (POSTER++)
2024: 85.8% (MAEL-FER)
2024: 96.5% (avec data synth√©tique*)
```

*Avec augmentation par Stable Diffusion

### Facteurs Cl√©s de Performance

1. **Qualit√© des donn√©es** > Architecture du mod√®le
2. **Pr√©-entra√Ænement** sur grands datasets (MS-Celeb-1M, VGGFace2)
3. **Augmentation de donn√©es** (mixup, cutout, synthetic)
4. **Gestion du bruit de labels** (EAC, SCN)
5. **Attention mechanisms** (POSTER, Transformers)

### Limitations Actuelles

- **Gap Lab/Wild**: Mod√®les CK+ ne g√©n√©ralisent pas bien
- **Biais culturels**: Datasets majoritairement occidentaux
- **√âmotions subtiles**: Disgust, Fear restent difficiles
- **Temps r√©el**: Trade-off accuracy vs latence

---

## R√©f√©rences

### Papers Cl√©s

1. [Facial Emotion Recognition: State of the Art Performance on FER2013](https://arxiv.org/abs/2105.03588) - arXiv 2021
2. [POSTER++: A simpler and stronger facial expression recognition network](https://arxiv.org/abs/2301.12149) - 2023
3. [AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild](https://arxiv.org/abs/1708.03985) - IEEE 2017
4. [Reliable Crowdsourcing and Deep Locality-Preserving Learning for Expression Recognition in the Wild](https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Reliable_Crowdsourcing_and_CVPR_2017_paper.pdf) - CVPR 2017 (RAF-DB)

### Leaderboards

- [Papers with Code - FER2013](https://paperswithcode.com/sota/facial-expression-recognition-on-fer2013)
- [Papers with Code - AffectNet](https://paperswithcode.com/sota/facial-expression-recognition-on-affectnet)
- [Papers with Code - RAF-DB](https://paperswithcode.com/sota/facial-expression-recognition-on-raf-db)

### Ressources Suppl√©mentaires

- [FER2013 Kaggle Dataset](https://www.kaggle.com/datasets/msambare/fer2013)
- [AffectNet Official](http://mohammadmahoor.com/databases-codes/)
- [RAF-DB Official](http://www.whdeng.cn/RAF/model1.html)
- [Improved facial emotion recognition model (2024)](https://www.nature.com/articles/s41598-024-79167-8)
- [Benchmarking deep networks for FER in the wild](https://link.springer.com/article/10.1007/s11042-022-12790-7)

---

*Cette documentation est g√©n√©r√©e pour le projet ProjectCare - Benchmark Emotion Detection*
